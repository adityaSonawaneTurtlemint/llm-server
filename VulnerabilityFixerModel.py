import json
import os
import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig
)
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from pathlib import Path
import javalang
from collections import defaultdict
from peft import PeftModel

os.environ["HUGGINGFACE_HUB_TOKEN"] = "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"

# ============================================================================
# AI MODEL FOR CODE GENERATION
# ============================================================================

@dataclass
class Fix:
    """Structure to hold generated fix"""
    vuln_id: str
    file_path: str
    original_code: str
    fixed_code: str
    explanation: str
    confidence_score: float
    affected_files: List[str]  # For multi-file changes


class VulnerabilityFixerModel:    
    def __init__(self, model_name: str, lora_path: Optional[str] = None):
        """
        Initialize the model
        
        Options:
        - codellama/CodeLlama-7b-hf (7B - faster)
        - codellama/CodeLlama-13b-hf (13B - better quality)
        - deepseek-ai/deepseek-coder-6.7b-base (good alternative)
        """
        print(f"Loading model: {model_name}")
        
        local_model_path = "../models/" + model_name

        # Configure 4-bit quantization for efficiency
        # bnb_config = BitsAndBytesConfig(
        #     load_in_4bit=True,
        #     bnb_4bit_use_double_quant=True,
        #     bnb_4bit_quant_type="nf4",
        #     bnb_4bit_compute_dtype=torch.float32
        # )
        
        print("Loading tokenizer and model...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=local_model_path, trust_remote_code=True, use_auth_token=os.environ["HUGGINGFACE_HUB_TOKEN"])
        print("Tokenizer loaded")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="cpu",
            trust_remote_code=True,
            cache_dir=local_model_path,
            use_auth_token=os.environ["HUGGINGFACE_HUB_TOKEN"]
        )
        print("Model loaded")
        
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # Attach LoRA fine-tuned adapters if provided
        if lora_path:
            print(f"Loading LoRA adapters from {lora_path}")
            self.model = PeftModel.from_pretrained(self.model, lora_path)

        print("Model initialization complete")
        
    def generate_fix(self, vuln, context, fix_patterns):
        """Generate a fix for the vulnerability"""
        
        # Build comprehensive prompt
        prompt = self._build_prompt(vuln, context, fix_patterns)
        prompt="VULNERABILITY DETAILS:\nType: SQL Injection\nCODE TO FIX:\n```java\nString query = \"SELECT * FROM users WHERE id = \" + userId;\n```\nTASK:\nFix it.\n"
        print(prompt)
        print(f"Generated prompt for vulnerability {vuln.vuln_id}:\n{prompt[:500]}...\n")
        # Generate fix
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        print("Generating fix...")
        with torch.no_grad():
            print("Generating with model...")
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                do_sample=False,
                num_return_sequences=1,
                repetition_penalty=1.1 # Recommended for code generation
            )
            print("Model generation done.")
        print("Generation complete.")
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"Generated text:\n{generated_text}\n")
        # Parse the generated fix
        fixed_code, explanation = self._parse_generated_fix(generated_text, prompt)
        
        # Calculate confidence score (simplified)
        confidence = self._calculate_confidence(fixed_code, context.target_code)
        
        return Fix(
            vuln_id=vuln.vuln_id,
            file_path=vuln.file_path,
            original_code=context.target_code,
            fixed_code=fixed_code,
            explanation=explanation,
            confidence_score=confidence,
            affected_files=[vuln.file_path]  # Could be extended for multi-file fixes
        )
    
    def _build_prompt(self, vuln, context, fix_patterns):
        # Include relevant fix patterns as examples
        examples_text = ""
        
        prompt = f"""You are a security expert fixing vulnerabilities in Java code.

            CURRENT CODE CONTEXT:
            Class: {context.class_name}
            Method: {context.method_name}

            Imports:
            {chr(10).join(context.imports[:10])}

            CODE TO FIX:
            ```java
            {context.target_code}
            ```

            TASK:
            1. Analyze the vulnerability in the code
            2. Generate the FIXED version of the code
            4. Preserve existing functionality while fixing the security issue

            OUTPUT FORMAT:
            FIXED_CODE:
            ```java
            [your fixed code here]
            ```
            Now generate the fix.
            """
        return prompt
    
    def _parse_generated_fix(self, generated_text: str, prompt: str) -> Tuple[str, str]:
        """Parse fixed code and explanation from generated text"""
        print("Parsing generated fix...")
        # Remove the prompt from generated text
        if prompt in generated_text:
            generated_text = generated_text[len(prompt):]
        
        # Extract fixed code
        fixed_code = ""
        explanation = ""
        
        # Look for FIXED_CODE section
        if "FIXED_CODE:" in generated_text:
            parts = generated_text.split("FIXED_CODE:")
            if len(parts) > 1:
                code_section = parts[1].split("EXPLANATION:")[0]
                # Extract code from markdown code block
                code_match = re.search(r'```java\n(.*?)\n```', code_section, re.DOTALL)
                if code_match:
                    fixed_code = code_match.group(1).strip()
        
        # Look for EXPLANATION section
        if "EXPLANATION:" in generated_text:
            parts = generated_text.split("EXPLANATION:")
            if len(parts) > 1:
                explanation = parts[1].strip()
        
        # Fallback: if no structured output, try to extract any code block
        if not fixed_code:
            code_match = re.search(r'```java\n(.*?)\n```', generated_text, re.DOTALL)
            if code_match:
                fixed_code = code_match.group(1).strip()
            else:
                fixed_code = generated_text.strip()
        
        return fixed_code, explanation
    
    def _calculate_confidence(self, fixed_code: str, original_code: str) -> float:
        """Calculate confidence score for the fix"""
        # Simplified confidence calculation
        # In production, use more sophisticated methods
        
        score = 0.5  # Base score
        
        # Check if code is non-empty and different from original
        if fixed_code and fixed_code != original_code:
            score += 0.2
        
        # Check for security-related improvements (heuristic)
        security_keywords = [
            'PreparedStatement', 'escapeHtml', 'sanitize', 'validate',
            'SecureRandom', 'SHA-256', 'getenv', 'whitelist'
        ]
        for keyword in security_keywords:
            if keyword in fixed_code and keyword not in original_code:
                score += 0.05
        
        # Check for proper Java syntax (basic check)
        if '{' in fixed_code and '}' in fixed_code:
            score += 0.1
        
        return min(score, 1.0)
